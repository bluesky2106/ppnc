@inproceedings{gehring2017convolutional,
  title={Convolutional sequence to sequence learning},
  author={Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
  booktitle={International Conference on Machine Learning},
  pages={1243--1252},
  year={2017},
  organization={PMLR}
}

@inproceedings{kalchbrenner-etal-2014-convolutional,
    title = "A Convolutional Neural Network for Modelling Sentences",
    author = "Kalchbrenner, Nal  and
      Grefenstette, Edward  and
      Blunsom, Phil",
    booktitle = "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P14-1062",
    doi = "10.3115/v1/P14-1062",
    pages = "655--665",
}

@inproceedings{kim-2014-convolutional,
    title = "Convolutional Neural Networks for Sentence Classification",
    author = "Kim, Yoon",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1181",
    doi = "10.3115/v1/D14-1181",
    pages = "1746--1751",
}

@inproceedings{sutskever-2014-sequence,
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
title = {Sequence to Sequence Learning with Neural Networks},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3104–3112},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{liu-2016-recurrent,
author = {Liu, Pengfei and Qiu, Xipeng and Huang, Xuanjing},
title = {Recurrent Neural Network for Text Classification with Multi-Task Learning},
year = {2016},
isbn = {9781577357704},
publisher = {AAAI Press},
abstract = {Neural network based methods have obtained great progress on a variety of natural language processing tasks. However, in most previous works, the models are learned based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we use the multitask learning framework to jointly learn across multiple related tasks. Based on recurrent neural network, we propose three different mechanisms of sharing information to model text with task-specific and shared layers. The entire network is trained jointly on all these tasks. Experiments on four benchmark text classification tasks show that our proposed models can improve the performance of a task with the help of other related tasks.},
booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
pages = {2873–2879},
numpages = {7},
location = {New York, New York, USA},
series = {IJCAI'16}
}

@inproceedings{socher-2013-recursivedeep,
    author = {Richard Socher and Alex Perelygin and Jean Y. Wu and Jason Chuang and Christopher D. Manning and Andrew Y. Ng and Christopher Potts},
    title = {Recursive deep models for semantic compositionality over a sentiment treebank},
    booktitle = {In Proceedings of EMNLP},
    year = {2013},
    pages = {1631--1642}
}

@inproceedings{tai2015improved,
  title={Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks},
  author={Tai, Kai Sheng and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={1556--1566},
  year={2015}
}

@inproceedings{marcheggiani2018exploiting,
  title={Exploiting Semantics in Neural Machine Translation with Graph Convolutional Networks},
  author={Marcheggiani, Diego and Bastings, Jasmijn and Titov, Ivan},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
  pages={486--492},
  year={2018}
}

@inproceedings{oquab2014learning,
  title={Learning and transferring mid-level image representations using convolutional neural networks},
  author={Oquab, Maxime and Bottou, Leon and Laptev, Ivan and Sivic, Josef},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1717--1724},
  year={2014}
}

@book{thrun2004advances,
  title={Advances in Neural Information Processing Systems 16: Proceedings of the 2003 Conference},
  author={Thrun, Sebastian and Saul, Lawrence K and Sch{\"o}lkopf, Bernhard},
  volume={16},
  year={2004},
  publisher={MIT press}
}

@article{minyoung2016ImageNet,
 author    = {Mi{-}Young Huh and
               Pulkit Agrawal and
               Alexei A. Efros},
  title     = {What makes ImageNet good for transfer learning?},
  journal   = {CoRR},
  volume    = {abs/1608.08614},
  year      = {2016},
  url       = {http://arxiv.org/abs/1608.08614},
  archivePrefix = {arXiv},
  eprint    = {1608.08614},
  timestamp = {Mon, 13 Aug 2018 16:46:41 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HuhAE16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Russakovsky2015ImageNet,
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
title = {ImageNet Large Scale Visual Recognition Challenge},
year = {2015},
issue_date = {December  2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {115},
number = {3},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-015-0816-y},
doi = {10.1007/s11263-015-0816-y},
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
journal = {Int. J. Comput. Vision},
month = dec,
pages = {211–252},
numpages = {42},
keywords = {Large-scale, Object recognition, Benchmark, Dataset, Object detection}
}

@inproceedings{Deng2009ImageNet,
  author={J. {Deng} and W. {Dong} and R. {Socher} and L. {Li} and  {Kai Li} and  {Li Fei-Fei}},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},
  title={ImageNet: A large-scale hierarchical image database},
  year={2009},
  volume={},
  number={},
  pages={248-255},
  doi={10.1109/CVPR.2009.5206848}
}

@article{kentonbert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  journal={Universal Language Model Fine-tuning for Text Classification},
  pages={278},
  year={2018}
}

@article{ dong2019unified,
  author    = {Li Dong and
               Nan Yang and
               Wenhui Wang and
               Furu Wei and
               Xiaodong Liu and
               Yu Wang and
               Jianfeng Gao and
               Ming Zhou and
               Hsiao{-}Wuen Hon},
  title     = {Unified Language Model Pre-training for Natural Language Understanding
               and Generation},
  journal   = {CoRR},
  volume    = {abs/1905.03197},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.03197},
  archivePrefix = {arXiv},
  eprint    = {1905.03197},
  timestamp = {Mon, 05 Oct 2020 12:53:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-03197.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{liu2019roberta,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  archivePrefix = {arXiv},
  eprint    = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{hestnessdeep,
  author    = {Joel Hestness and
               Sharan Narang and
               Newsha Ardalani and
               Gregory F. Diamos and
               Heewoo Jun and
               Hassan Kianinejad and
               Md. Mostofa Ali Patwary and
               Yang Yang and
               Yanqi Zhou},
  title     = {Deep Learning Scaling is Predictable, Empirically},
  journal   = {CoRR},
  volume    = {abs/1712.00409},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.00409},
  archivePrefix = {arXiv},
  eprint    = {1712.00409},
  timestamp = {Mon, 13 Aug 2018 16:48:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1712-00409.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{shazeer2017outrageously,
    author    = {Noam Shazeer and
               Azalia Mirhoseini and
               Krzysztof Maziarz and
               Andy Davis and
               Quoc V. Le and
               Geoffrey E. Hinton and
               Jeff Dean},
  title     = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts
               Layer},
  journal   = {CoRR},
  volume    = {abs/1701.06538},
  year      = {2017},
  url       = {http://arxiv.org/abs/1701.06538},
  archivePrefix = {arXiv},
  eprint    = {1701.06538},
  timestamp = {Mon, 13 Aug 2018 16:46:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ShazeerMMDLHD17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{jozefowicz2016exploring,
    author    = {Rafal J{\'{o}}zefowicz and
               Oriol Vinyals and
               Mike Schuster and
               Noam Shazeer and
               Yonghui Wu},
  title     = {Exploring the Limits of Language Modeling},
  journal   = {CoRR},
  volume    = {abs/1602.02410},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.02410},
  archivePrefix = {arXiv},
  eprint    = {1602.02410},
  timestamp = {Mon, 13 Aug 2018 16:48:43 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/JozefowiczVSSW16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mahajan2018exploring,
 author    = {Dhruv Mahajan and
               Ross B. Girshick and
               Vignesh Ramanathan and
               Kaiming He and
               Manohar Paluri and
               Yixuan Li and
               Ashwin Bharambe and
               Laurens van der Maaten},
  editor    = {Vittorio Ferrari and
               Martial Hebert and
               Cristian Sminchisescu and
               Yair Weiss},
  title     = {Exploring the Limits of Weakly Supervised Pretraining},
  booktitle = {Computer Vision - {ECCV} 2018 - 15th European Conference, Munich,
               Germany, September 8-14, 2018, Proceedings, Part {II}},
  series    = {Lecture Notes in Computer Science},
  volume    = {11206},
  pages     = {185--201},
  publisher = {Springer},
  year      = {2018},
  url       = {https://doi.org/10.1007/978-3-030-01216-8\_12},
  doi       = {10.1007/978-3-030-01216-8\_12},
  timestamp = {Mon, 28 Sep 2020 08:19:37 +0200},
  biburl    = {https://dblp.org/rec/conf/eccv/MahajanGRHPLBM18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
